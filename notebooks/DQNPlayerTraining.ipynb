{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.utils.card_utils import Card, Deck\n",
    "from pypokerengine.api.game import setup_config, start_poker\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "\n",
    "import PlayerModels as pm\n",
    "#from MyEmulator import MyEmulator\n",
    "#from DQNPlayer import DQNPlayer\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target_graph(tf_vars, tau):\n",
    "    total_vars = len(tf_vars)\n",
    "    ops = []\n",
    "    for i, var in enumerate(tf_vars[0:total_vars // 2]):\n",
    "        ops.append(tf_vars[i + total_vars // 2].assign((var.value() * tau) +\n",
    "                                                       (tf_vars[i + total_vars // 2].value() * (1 - tau))))\n",
    "    return ops\n",
    "\n",
    "def update_target(ops, sess):\n",
    "    for op in ops:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "update_freq = 50 # how often to update model\n",
    "y = 0.99 # discount\n",
    "start_E = 1 # starting chance of random action\n",
    "end_E = 0.2 # final chance of random action\n",
    "annealings_steps = 100000 # how many steps to reduce start_E to end_E\n",
    "num_episodes = 5000\n",
    "pre_train_steps = 5000 # how many steps of random action before training begin\n",
    "load_model = False\n",
    "path = '../cache/models/DQN/'\n",
    "h_size = 128 # the size of final conv layer before spliting it into advantage and value streams\n",
    "tau = 0.01 # rate to update target network toward primary network\n",
    "is_dueling = True # whether or not to use dueling architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emul = MyEmulator()\n",
    "emul.set_game_rule(9, 50, 15, 0)\n",
    "my_uuid = '9'\n",
    "players_info = {\n",
    "    \"1\": { \"name\": \"f1\", \"stack\": 1500 },\n",
    "    \"2\": { \"name\": \"f2\", \"stack\": 1500 },\n",
    "    \"3\": { \"name\": \"f3\", \"stack\": 1500 },\n",
    "    \"4\": { \"name\": \"f4\", \"stack\": 1500 },\n",
    "    \"5\": { \"name\": \"f5\", \"stack\": 1500 },\n",
    "    \"6\": { \"name\": \"f6\", \"stack\": 1500 },\n",
    "    \"7\": { \"name\": \"f7\", \"stack\": 1500 },\n",
    "    \"8\": { \"name\": \"f8\", \"stack\": 1500 },\n",
    "    \"9\": { \"name\": \"f9\", \"stack\": 1500 }\n",
    "}\n",
    "\n",
    "def init_emul(my_uuid_):\n",
    "    global my_uuid\n",
    "    my_uuid = my_uuid_\n",
    "    \n",
    "    emul.register_player(\"1\", pm.CallPlayer())\n",
    "    emul.register_player(\"2\", pm.CallPlayer())\n",
    "    emul.register_player(\"3\", pm.FoldPlayer())\n",
    "    emul.register_player(\"4\", pm.FoldPlayer())\n",
    "    emul.register_player(\"5\", pm.HeuristicPlayer())\n",
    "    emul.register_player(\"6\", pm.HeuristicPlayer())\n",
    "    emul.register_player(\"7\", pm.RandomPlayer())\n",
    "    emul.register_player(\"8\", pm.RandomPlayer())\n",
    "    emul.register_player(\"9\", pm.CallPlayer())\n",
    "\n",
    "\n",
    "    players_info = {\n",
    "        \"1\": { \"name\": \"CallPlayer1\", \"stack\": 1500 },\n",
    "        \"2\": { \"name\": \"CallPlayer2\", \"stack\": 1500 },\n",
    "        \"3\": { \"name\": \"FoldPlayer1\", \"stack\": 1500 },\n",
    "        \"4\": { \"name\": \"FoldPlayer2\", \"stack\": 1500 },\n",
    "        \"5\": { \"name\": \"HeuristicPlayer1\", \"stack\": 1500 },\n",
    "        \"6\": { \"name\": \"HeuristicPlayer2\", \"stack\": 1500 },\n",
    "        \"7\": { \"name\": \"RandomPlayer1\", \"stack\": 1500 },\n",
    "        \"8\": { \"name\": \"RandomPlayer2\", \"stack\": 1500 },\n",
    "        \"9\": { \"name\": \"DQN\", \"stack\": 1500 }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "main_wp = DQNPlayer(h_size, is_double=True)\n",
    "target_wp = DQNPlayer(h_size, is_main=False, is_double=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n",
      "0 23 3.66837180629 1 2.0\n",
      "100 1536 27.9723995198 1 2.0\n",
      "200 2814 6.94813026128 1 2.0\n",
      "300 4227 31.137493052 1 2.0\n",
      "400 5799 15.1761009621 0.9936079999999936 2.0\n",
      "500 7214 7.6173269191 0.9822879999999823 2.0\n",
      "600 8581 23.6309051878 0.9713519999999713 2.0\n",
      "700 10044 26.773214296 0.9596479999999596 2.0\n",
      "800 11740 4.68016781608 0.9460799999999461 2.0\n",
      "900 12910 -8.50774757873 0.9367199999999367 2.0\n",
      "Saved model\n",
      "1000 14321 31.247689777 0.9254319999999254 2.0\n",
      "1100 15534 -9.27358378824 0.9157279999999157 2.0\n",
      "1200 16902 22.4812399899 0.9047839999999048 2.0\n",
      "1300 18331 36.6012055903 0.8933519999998933 2.0\n",
      "1400 19917 6.68827601624 0.8806639999998807 2.0\n",
      "1500 21456 21.9802272021 0.8683519999998683 2.0\n",
      "1600 22770 30.6701865034 0.8578399999998578 2.0\n",
      "1700 24384 8.41407082864 0.8449279999998449 2.0\n",
      "1800 25752 -11.2434144055 0.833983999999834 2.0\n",
      "1900 27279 12.4470851459 0.8217679999998218 2.0\n",
      "Saved model\n",
      "2000 28706 -13.9231750384 0.8103519999998103 2.0\n",
      "2100 30051 47.2092057226 0.7995919999997996 2.0\n",
      "2200 31309 0.706339614822 0.7895279999997895 2.0\n",
      "2300 32611 -10.9169764499 0.7791119999997791 2.0\n",
      "2400 34150 7.20594666598 0.7667999999997668 2.0\n",
      "2500 35627 -9.74989425886 0.754983999999755 2.0\n",
      "2600 37089 -12.2588142688 0.7432879999997433 1.0\n",
      "2700 38570 3.17869832587 0.7314399999997314 2.0\n",
      "2800 40149 6.39183211873 0.7188079999997188 2.0\n",
      "2900 41382 12.3417195245 0.7089439999997089 2.0\n",
      "Saved model\n",
      "3000 42994 -9.7374967758 0.696047999999696 2.0\n",
      "3100 44920 46.1625095924 0.6806399999996806 2.0\n",
      "3200 46526 27.8401346303 0.6677919999996678 2.0\n",
      "3300 48347 31.9511279559 0.6532239999996532 2.0\n",
      "3400 49718 -11.2953002263 0.6422559999996422 2.0\n",
      "3600 53219 28.2689318283 0.6142479999996142 2.0\n",
      "3700 54374 -8.33971895279 0.605007999999605 2.0\n",
      "Mean reward: 14.657275679876504\n",
      "CPU times: user 11h 57min 1s, sys: 10 s, total: 11h 57min 11s\n",
      "Wall time: 11h 48min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "trainables = tf.trainable_variables()\n",
    "target_ops = update_target_graph(trainables, tau)\n",
    "my_buffer = ExperienceBuffer()\n",
    "\n",
    "e = start_E\n",
    "step_drop = (start_E - end_E) / annealings_steps\n",
    "\n",
    "j_list = []\n",
    "r_list = []\n",
    "action_list = []\n",
    "total_steps = 0\n",
    "\n",
    "tf.device(\"/gpu:0\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model:\n",
    "        print('Loading model')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episode_buffer = ExperienceBuffer()\n",
    "        init_emul(str(np.random.randint(1, 10)))\n",
    "        \n",
    "        initial_state = emul.generate_initial_game_state(players_info)\n",
    "        msgs = []\n",
    "        game_state, events = emul.start_new_round(initial_state)\n",
    "        is_last_round = False\n",
    "        r_all = 0\n",
    "        j = 0\n",
    "        \n",
    "        last_img_state = None\n",
    "        last_features = None\n",
    "        last_action_num = None\n",
    "        \n",
    "        while not is_last_round:\n",
    "            j += 1\n",
    "            a = emul.run_until_my_next_action(game_state, my_uuid, msgs)\n",
    "            \n",
    "            # need to make move\n",
    "            if len(a) == 4:\n",
    "                game_state, valid_actions, hole_card, round_state = a\n",
    "                img_state = img_from_state(hole_card, round_state)\n",
    "                img_state = process_img(img_state)\n",
    "                \n",
    "                street = round_state['street']\n",
    "                bank = round_state['pot']['main']['amount']\n",
    "                stack = [s['stack'] for s in round_state['seats'] if s['uuid'] == my_uuid][0]\n",
    "                other_stacks = [s['stack'] for s in round_state['seats'] if s['uuid'] != my_uuid]\n",
    "                dealer_btn = round_state['dealer_btn']\n",
    "                small_blind_pos = round_state['small_blind_pos']\n",
    "                big_blind_pos = round_state['big_blind_pos']\n",
    "                next_player = round_state['next_player']\n",
    "                round_count = round_state['round_count']\n",
    "                estimation = main_wp.hole_card_est[(hole_card[0], hole_card[1])]\n",
    "\n",
    "                features = get_street(street)\n",
    "                features.extend([bank, stack, dealer_btn, small_blind_pos, big_blind_pos, next_player, round_count])\n",
    "                features.extend(other_stacks)\n",
    "                features.append(estimation)\n",
    "                        \n",
    "                # add to buffer last hand \n",
    "                if last_img_state is not None:\n",
    "                    episode_buffer.add(np.reshape(np.array([last_img_state, last_features, last_action_num,\n",
    "                                                            0, img_state, features, 0]), [1, 7]))\n",
    "                \n",
    "                if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                    action_num = np.random.randint(0, main_wp.total_num_actions)\n",
    "                else:\n",
    "                    action_num = sess.run(main_wp.predict, feed_dict={main_wp.scalar_input: [img_state],\n",
    "                                                                      main_wp.features_input: [features]})[0]\n",
    "                    \n",
    "                action_list.append(action_num)\n",
    "                action, amount = get_action_by_num(action_num, valid_actions)                    \n",
    "\n",
    "                game_state, msgs = emul.apply_my_action(game_state, action, amount)\n",
    "                total_steps += 1\n",
    "        \n",
    "                last_img_state = img_state.copy()\n",
    "                last_features = features.copy()\n",
    "                last_action_num = action_num\n",
    "                \n",
    "                if total_steps > pre_train_steps:\n",
    "                    if e > end_E:\n",
    "                        e -= step_drop\n",
    "                    \n",
    "                    if total_steps % (update_freq) == 0:\n",
    "                        train_batch = my_buffer.sample(batch_size)\n",
    "\n",
    "                        Q1 = sess.run(main_wp.predict,\n",
    "                                      feed_dict={main_wp.scalar_input: np.vstack(train_batch[:, 4]),\n",
    "                                                 main_wp.features_input: np.vstack(train_batch[:, 5])})\n",
    "                        Q1_ = sess.run(main_wp.Q_out,\n",
    "                                      feed_dict={main_wp.scalar_input: np.vstack(train_batch[:, 4]),\n",
    "                                                 main_wp.features_input: np.vstack(train_batch[:, 5])})\n",
    "        \n",
    "                        Q2 = sess.run(target_wp.Q_out,\n",
    "                                      feed_dict={target_wp.scalar_input: np.vstack(train_batch[:, 4]),\n",
    "                                                 target_wp.features_input: np.vstack(train_batch[:, 5])})\n",
    "                        end_multiplier = -(train_batch[:, 6] - 1)\n",
    "                        double_Q = Q2[range(batch_size), Q1]\n",
    "                        double_Q_ = Q1_[range(batch_size), Q1]\n",
    "                        \n",
    "                        if is_dueling:\n",
    "                            target_Q = train_batch[:, 3] + (y * double_Q * end_multiplier)\n",
    "                        else:\n",
    "                            target_Q = train_batch[:, 3] + (y * double_Q_ * end_multiplier)\n",
    "\n",
    "                        _, er, g, v = sess.run([main_wp.update_model,\n",
    "                                          main_wp.loss, main_wp.grad_norms, main_wp.var_norms],\n",
    "                                     feed_dict={\n",
    "                                         main_wp.scalar_input: np.vstack(train_batch[:, 0]),\n",
    "                                         main_wp.features_input: np.vstack(train_batch[:, 1]),\n",
    "                                         main_wp.target_Q: target_Q,\n",
    "                                         main_wp.actions: train_batch[:, 2]\n",
    "                                     })\n",
    "                        update_target(target_ops, sess)\n",
    "                        \n",
    "                        r = np.mean(r_list[-2:])\n",
    "                        j = np.mean(j_list[-2:])\n",
    "                        q1 = double_Q_[0]\n",
    "                        q2 = double_Q[0]\n",
    "                        al = np.mean(action_list[-10:])\n",
    "                                                \n",
    "                        summary = tf.Summary()\n",
    "                        summary.value.add(tag='Perf/Reward', simple_value=float(r))\n",
    "                        summary.value.add(tag='Perf/Lenght', simple_value=float(j))\n",
    "                        summary.value.add(tag='Perf/Action_list', simple_value=al)\n",
    "                        summary.value.add(tag='Perf/E', simple_value=e)                        \n",
    "                        summary.value.add(tag='Q/Q1', simple_value=float(q1))\n",
    "                        summary.value.add(tag='Q/Q2', simple_value=float(q2))\n",
    "                        summary.value.add(tag='Q/Target', simple_value=target_Q[0])\n",
    "                        summary.value.add(tag='Q/Action', simple_value=Q1[0])\n",
    "                        summary.value.add(tag='Loss/Error', simple_value=er)\n",
    "                        summary.value.add(tag='Loss/Grad_norm', simple_value=g)\n",
    "                        summary.value.add(tag='Loss/Var_norm', simple_value=v)\n",
    "                        \n",
    "                        main_wp.summary_writer.add_summary(summary, total_steps)\n",
    "                        if total_steps % (update_freq * 2) == 0:\n",
    "                            main_wp.summary_writer.flush()                        \n",
    "            else:\n",
    "                game_state, reward = a\n",
    "                if reward >= 0:\n",
    "                    reward = np.log(1 + reward)\n",
    "                else:\n",
    "                    reward = -np.log(1 - reward)\n",
    "                r_all += reward\n",
    "                # add to buffer last hand \n",
    "                if last_img_state is not None:\n",
    "                    episode_buffer.add(np.reshape(np.array([last_img_state, last_features, last_action_num,\n",
    "                                                            reward, last_img_state, last_features, 1]), [1, 7]))\n",
    "                \n",
    "                is_last_round = emul._is_last_round(game_state, emul.game_rule)\n",
    "                game_state, events = emul.start_new_round(game_state)\n",
    "\n",
    "                last_img_state = None\n",
    "                last_action_num = None   \n",
    "            \n",
    "        my_buffer.add(episode_buffer.buffer)\n",
    "        r_list.append(r_all)\n",
    "        j_list.append(j)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess, path + '/model_' + str(i) + '.ckpt')\n",
    "            saver.save(sess,path, i)\n",
    "\n",
    "            print('Saved model')\n",
    "        if i % 100 == 0:\n",
    "            \n",
    "            print(i, total_steps, np.mean(r_list[-10:]), e, np.median(action_list[-200:]))            \n",
    "    saver.save(sess, path + '/model_' + str(i) + '.ckpt')\n",
    "print('Mean reward: {}'.format(sum(r_list) / num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the training progress type in console:\n",
    "tensorboard --logdir=log/DQN/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on small(to debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# init = tf.global_variables_initializer()\n",
    "# trainables = tf.trainable_variables()\n",
    "# target_ops = update_target_graph(trainables, tau)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     if load_model:\n",
    "#         print('Loading model')\n",
    "#         ckpt = tf.train.get_checkpoint_state(path)\n",
    "#         saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#     train_batch = my_buffer.sample(batch_size)\n",
    "#     for i in range(num_episodes):      \n",
    "#         Q1 = sess.run(main_wp.predict,\n",
    "#                       feed_dict={main_wp.scalar_input: np.vstack(train_batch[:, 4]),\n",
    "#                                  main_wp.features_input: np.vstack(train_batch[:, 5])})\n",
    "#         Q1_ = sess.run(main_wp.Q_out,\n",
    "#                       feed_dict={main_wp.scalar_input: np.vstack(train_batch[:, 4]),\n",
    "#                                  main_wp.features_input: np.vstack(train_batch[:, 5])})\n",
    "\n",
    "#         Q2 = sess.run(target_wp.Q_out,\n",
    "#                       feed_dict={target_wp.scalar_input: np.vstack(train_batch[:, 4]),\n",
    "#                                  target_wp.features_input: np.vstack(train_batch[:, 5])})\n",
    "#         end_multiplier = -(train_batch[:, 6] - 1)\n",
    "#         double_Q = Q2[range(batch_size), Q1]\n",
    "#         double_Q_ = Q1_[range(batch_size), Q1]\n",
    "\n",
    "#         target_Q = train_batch[:, 3] + (y * double_Q * end_multiplier)\n",
    "\n",
    "#         _, er, g, v = sess.run([main_wp.update_model,\n",
    "#                           main_wp.loss, main_wp.grad_norms, main_wp.var_norms],\n",
    "#                      feed_dict={\n",
    "#                          main_wp.scalar_input: np.vstack(train_batch[:, 0]),\n",
    "#                          main_wp.features_input: np.vstack(train_batch[:, 1]),\n",
    "#                          main_wp.target_Q: target_Q,\n",
    "#                          main_wp.actions: train_batch[:, 2]\n",
    "#                      })\n",
    "#         update_target(target_ops, sess)\n",
    "        \n",
    "#         q1 = double_Q_[0]\n",
    "#         q2 = double_Q[0]\n",
    "\n",
    "#         summary = tf.Summary()\n",
    "#         summary.value.add(tag='Q/Q1', simple_value=float(q1))\n",
    "#         summary.value.add(tag='Q/Q2', simple_value=float(q2))\n",
    "#         summary.value.add(tag='Q/Target', simple_value=target_Q[0])\n",
    "#         summary.value.add(tag='Q/Action', simple_value=Q1[0])\n",
    "#         summary.value.add(tag='Loss/Error', simple_value=er)\n",
    "#         summary.value.add(tag='Loss/Grad_norm', simple_value=g)\n",
    "#         summary.value.add(tag='Loss/Var_norm', simple_value=v)\n",
    "\n",
    "#         main_wp.summary_writer.add_summary(summary, i)\n",
    "#         if i % 2 == 0:\n",
    "#             main_wp.summary_writer.flush()\n",
    "                            \n",
    "#         if i % 100 == 0:\n",
    "#             print(i, total_steps, np.mean(r_list[-10:]), e, np.median(action_list[-200:]))            \n",
    "# print('Mean reward: {}'.format(sum(r_list) / num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vars_ = tf.trainable_variables()[:len(tf.trainable_variables()) // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 32)\n",
      "4\n",
      "5\n",
      "5\n",
      "1\n",
      "32\n",
      "800\n",
      "(32,)\n",
      "1\n",
      "32\n",
      "32\n",
      "(3, 3, 32, 64)\n",
      "4\n",
      "3\n",
      "3\n",
      "32\n",
      "64\n",
      "18432\n",
      "(64,)\n",
      "1\n",
      "64\n",
      "64\n",
      "(5, 5, 64, 128)\n",
      "4\n",
      "5\n",
      "5\n",
      "64\n",
      "128\n",
      "204800\n",
      "(128,)\n",
      "1\n",
      "128\n",
      "128\n",
      "(20, 64)\n",
      "2\n",
      "20\n",
      "64\n",
      "1280\n",
      "(64,)\n",
      "1\n",
      "64\n",
      "64\n",
      "(64, 128)\n",
      "2\n",
      "64\n",
      "128\n",
      "8192\n",
      "(128,)\n",
      "1\n",
      "128\n",
      "128\n",
      "(256, 256)\n",
      "2\n",
      "256\n",
      "256\n",
      "65536\n",
      "(256,)\n",
      "1\n",
      "256\n",
      "256\n",
      "(256, 128)\n",
      "2\n",
      "256\n",
      "128\n",
      "32768\n",
      "(128,)\n",
      "1\n",
      "128\n",
      "128\n",
      "(64, 5)\n",
      "2\n",
      "64\n",
      "5\n",
      "320\n",
      "(64, 1)\n",
      "2\n",
      "64\n",
      "1\n",
      "64\n",
      "332992\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in vars_:\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time of one pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    print(ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    action_num = sess.run(main_wp.predict, feed_dict={main_wp.scalar_input: [train_batch[3][0]],\n",
    "                                                      main_wp.features_input: [train_batch[3][1]]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch = my_buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cache/models/DQN/model_4999.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../cache/models/DQN/model_4999.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.Session() as sess:\n",
    "    qs1 = []\n",
    "    qs2 = []\n",
    "    qs3 = []\n",
    "    qs4 = []\n",
    "    qs5 = []\n",
    "\n",
    "    sess.run(init)\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    print(ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path) # 'models/model_197000.ckpt')\n",
    "    \n",
    "    bank_range = np.linspace(-1000, 20000, 100)\n",
    "    stack_range = np.linspace(-1000, 10000, 100)\n",
    "    probability_range = np.linspace(-0.5, 1.5, 100)\n",
    "    round_count_range = np.linspace(-10, 50, 100)\n",
    "    player_pos_range = np.linspace(-1, 11, 100)\n",
    "\n",
    "    for b in range(len(train_batch)):\n",
    "        batch = train_batch[b]\n",
    "        img = batch[0]\n",
    "        features = batch[1]\n",
    "        \n",
    "        q1 = []\n",
    "        q2 = []\n",
    "        q3 = []\n",
    "        q4 = []\n",
    "        q5 = []\n",
    "        \n",
    "        for i in range(100):\n",
    "            features = batch[1].copy()\n",
    "            features[4] = bank_range[i]\n",
    "            action_num = sess.run(main_wp.Q_out, feed_dict={main_wp.scalar_input: [img],\n",
    "                                                          main_wp.features_input: [features]})[0]\n",
    "            q1.append(action_num)\n",
    "\n",
    "            features = batch[1].copy()\n",
    "            features[5] = stack_range[i]\n",
    "            action_num = sess.run(main_wp.Q_out, feed_dict={main_wp.scalar_input: [img],\n",
    "                                                          main_wp.features_input: [features]})[0]\n",
    "            q2.append(action_num)\n",
    "\n",
    "            features = batch[1].copy()\n",
    "            features[-1] = probability_range[i]\n",
    "            action_num = sess.run(main_wp.Q_out, feed_dict={main_wp.scalar_input: [img],\n",
    "                                                          main_wp.features_input: [features]})[0]\n",
    "            q3.append(action_num)\n",
    "\n",
    "            features = batch[1].copy()\n",
    "            features[-10] = round_count_range[i]\n",
    "            action_num = sess.run(main_wp.Q_out, feed_dict={main_wp.scalar_input: [img],\n",
    "                                                          main_wp.features_input: [features]})[0]\n",
    "            q4.append(action_num)\n",
    "            \n",
    "            features = batch[1].copy()\n",
    "            features[-9] = player_pos_range[i]\n",
    "            action_num = sess.run(main_wp.Q_out, feed_dict={main_wp.scalar_input: [img],\n",
    "                                                          main_wp.features_input: [features]})[0]\n",
    "            q5.append(action_num)\n",
    "        \n",
    "        if b == 0:\n",
    "            qs1 = np.array(q1)\n",
    "            qs2 = np.array(q2)\n",
    "            qs3 = np.array(q3)\n",
    "            qs4 = np.array(q4)\n",
    "            qs5 = np.array(q5)\n",
    "        else:\n",
    "            qs1 += np.array(q1)\n",
    "            qs2 += np.array(q2)\n",
    "            qs3 += np.array(q3)\n",
    "            qs4 += np.array(q4) \n",
    "            qs5 += np.array(q5) \n",
    "\n",
    "    qs1 /= len(train_batch)\n",
    "    qs2 /= len(train_batch)\n",
    "    qs3 /= len(train_batch)\n",
    "    qs4 /= len(train_batch)      \n",
    "    qs5 /= len(train_batch)      \n",
    "\n",
    "    df1 = pd.DataFrame(qs1, columns=['fold', 'call', 'raise_min', 'raise_max', 'raise_half'])\n",
    "    df1.plot(bank_range, ['fold', 'call', 'raise_min', 'raise_max', 'raise_half'], title='bank_range')\n",
    "\n",
    "    df2 = pd.DataFrame(qs2, columns=['fold', 'call', 'raise_min', 'raise_max', 'raise_half'])\n",
    "    df2.plot(stack_range, ['fold', 'call', 'raise_min', 'raise_max', 'raise_half'], title='stack_range')\n",
    "\n",
    "    df3 = pd.DataFrame(qs3, columns=['fold', 'call', 'raise_min', 'raise_max', 'raise_half'])\n",
    "    df3.plot(probability_range, ['fold', 'call', 'raise_min', 'raise_max', 'raise_half'], title='probability')\n",
    "    \n",
    "    df4 = pd.DataFrame(qs4, columns=['fold', 'call', 'raise_min', 'raise_max', 'raise_half'])\n",
    "    df4.plot(round_count_range, ['fold', 'call', 'raise_min', 'raifse_max', 'raise_half'], title='round_count')\n",
    "    \n",
    "    df5 = pd.DataFrame(qs5, columns=['fold', 'call', 'raise_min', 'raise_max', 'raise_half'])\n",
    "    df5.plot(player_pos_range, ['fold', 'call', 'raise_min', 'raise_max', 'raise_half'], title='player_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist with probability win in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = [t[1][-1] for t in train_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pr, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'conv2d/kernel')\n",
    "d = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dense/kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = kernel[0]\n",
    "d = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    c1 = k.eval(session=sess)\n",
    "    d1 = d.eval(session=sess)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.shape, d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(d1)\n",
    "plt.gray()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c1[:, :, 0, 1])\n",
    "plt.gray()\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
